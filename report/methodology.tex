In this chapter we first discuss the data collection and processing steps
involved to generate a database of samples to be used for training and testing.
In Section~\ref{sec:method:feat_extraction} we explore the different options for
feature representation and lay out some experiments to be conducted. In
Section~\ref{sec:training} we explore the structure of the models used for
classification and discuss how training and/or optimisation will be performed.
We finish the chapter by commenting on how the models will be evaluated.

All computational work for this thesis was performed in MATLAB vR2023a
unless stated otherwise.

\section{Collecting the datasets}

Since all the models described in Section~\ref{sec:classification} are trained
in a supervised learning fashion, this requires a dataset of labelled training
examples. The website xeno-canto\footnote{\url{http://xeno-canto.org}} (XC) houses the
largest and most comprehensive publicly available collection of birdsong samples
in the world. It has made an enormous impact in the field of birdsong recognition
since its inception in 2005 and has been the source of the datasets for the
annual BirdCLEF challenge since 2014~\cite{vellinga2015xeno}. All samples
available on XC are labelled with the bird species and contain rich metadata,
such as the time and location of the recording. Importantly, all samples have a
crowd-sourced rating from A --- E which signifies how clear the recording sample
is, where A denotes samples with the highest quality and clarity and E denotes
the lowest. This is especially important should we wish to experiment with
different signal-to-noise ratios (SNR) as the noise can be manually added to a
clean recording at precise SNR levels.

Due to its scale and reputation in the birdsong classification community, all
samples used in this thesis have been downloaded from the XC repository. All
samples downloaded have an A rating and have been labelled as a `song' rather
than a `call'. The samples are stored in a directory labelled according to the
first three letters of the species' binomial name. For example, samples from the
common blackbird (\textit{Turdus merula}) are stored in a directory called
`TURMER'. This directory name acts as a label for training and test samples.

In this work we are mostly concerned with testing relative improvements in model
performance after tuning various components, such as novel feature
representations. To this end, we select the most simple form of classification
experiment: binary classification. The two bird classes selected for all
experiments herein are the common blackbird and the common nightingale
(\textit{Luscinia megarhynchos}) due to the two birds' vocalisations being
acoustically similar and hence more difficult to differentiate resulting in more
variable accuracy results, and the author's personal preference.

Roughly 38 individuals from each class were downloaded, with 28 used for
training and the remaining kept for testing. All samples were checked for
suitability and downloaded manually.

\section{Pre-processing audio recordings}

Some recordings available on XC are recorded in stereo sound, so to
reduce dimensionality without losing too much information, all stereo recordings
are first converted to mono by taking a mean average of both channels. Leading
and trailing sections of background noise are then stripped from the recording
using the \texttt{detectSpeech} function provided by MATLAB\@. This
function uses a thresholding algorithm to detect onset and offset indices of
speech~\cite{giannakopoulos2009method}. The function works for birdsong since
birdsong frequencies reside in a similar range to that of human speech. The
function accepts various arguments to determine properties such as window length
and threshold value, but from initial experiments the function was shown to
work well with birdsong using the defaults, see Figure~\ref{fig:detected_speech}.
The detected onset and offset of birdsong allows for the leading and trailing
bits of background noise to be removed from the recording, thus reducing
unnecessary computational time in segmenting the audio.

\begin{figure}[ht]
  \centering
  \includegraphics[width=\textwidth]{figures/detected_speech.png}
  \caption{Output of the \texttt{detectSpeech} function for a recording of
  birdsong from a Eurasian wren. The highlighted section shows where the
algorithm has detected `speech', or in this case,
birdsong.}\label{fig:detected_speech}
\end{figure}

A highpass filter is then applied to the recording with a passband frequency of
450 Hz using MATLAB's \texttt{highpass} function. This value was chosen
as most birdsong sits in the range of 1 kHz --- 10 kHz, and so lower frequencies
can be attributed to background noise and therefore their removal should reduce
noise in the system without losing important information. The reason why
frequencies above 10 kHz are not removed is that, as can be seen from
Figure~\ref{fig:wren_call_song_spectrogram}, birds typically produce higher
frequency harmonics when vocalising, shown as vertical bands moving upwards from
where syllables are located on the spectrogram. These harmonics may capture
useful information for the model to utilise and should therefore not be filtered
out.

After these pre-processing steps have been taken, the audio is ready for
syllable segmentation.

\section{Segmenting audio recordings}\label{sec:segmentation}

Recordings downloaded from XC were also chosen based on their length. Recordings
of duration 40 seconds to 120 seconds were preferred since they were likely to
be long enough to include enough variety of vocalisations from one individual
bird, but not too long so as to take up too much space on disk. In this thesis
it was preferred to have fewer samples from more individuals as opposed to more
samples from fewer individuals for each species. The intention of this was to
introduce more variation in the training samples for each class, especially when
considering that many birds of the same species but residing in different
locations have demonstrated subtle variations in vocalisations, known as
dialects~\cite{baker1985biology}. As a general rule of thumb, a target of
roughly 50 training samples per individual was aimed for in this thesis.

This leads to the question of how to generate samples. In the literature there
seem to be two main ways of segmenting recordings into samples to be used for
training. The first is segmenting a recording into overlapping segments of a
certain length, typically in the range of 4 --- 11
seconds~\cite{yan2021birdsong,crous2019polyphonic}. This has the advantage that
all samples will be of fixed length and that the segmentation algorithm is very
simple. However, it will likely mean that some samples will contain only
background noise which, if used for training, will introduce noise into the
system. These noise samples therefore may need to be removed, either
manually~\cite{yan2021birdsong} or using an
algorithm~\cite{narasimhan2017simultaneous}. The other method involves
segmenting the recording into
syllables~\cite{fagerlund2007bird,ramashini2022robust}. This has the advantage
that all training samples are likely to contain little or no noise. However the
samples will be of variable length, so steps will be needed to be taken 
to compare the samples, such as adding padding or more advanced techniques like
dynamic time warping~\cite{somervuo2006parametric}. Syllable segmentation also
has the enticing prospect of being able to identify a bird species from a very
short sample. This could be useful in situations where a recording is mostly
corrupted by background noise but has a few small segments of clear birdsong.

In this work we attempt to combine the benefits of both approaches by first
segmenting a recording to retrieve the syllables, then combining the syllables
to form a sequence. The following sections will describe both processes in
detail.

\subsection{Energy based syllable segmentation}\label{ssec:syllable_seg}

This algorithm was first proposed by Fagerlund~\cite{fagerlund2004automatic} and
has been used in various research papers
since~\cite{somervuo2006parametric,ramashini2022robust}. Note that Fagerlund
proposed two different segmentation methods in the paper. In this thesis we use
the energy based segmentation method as it's relatively simple to implement and
its performance can be superior to the other method proposed. At the time of
writing there was no open source code available to perform this algorithm so it
was written from scratch for this thesis. The algorithm is computed in the
following 3 phases:

\begin{enumerate}

  \item Determine the onsets and offsets of syllable candidates. Special
    attention has to be paid to the border effect~\cite{li2001classification},
    whereby the energy envelope near onsets and offsets fluctuates around a
    threshold.

  \item Syllable candidates that are within a certain temporal distance with
    each other are merged into one. The initial separation of candidates can
    likely be attributed to the border effect, and so the candidates can be
    thought of as originating from one syllable in reality.

  \item Syllable candidates that are shorter than a minimum threshold are
    removed. These candidates are often caused by random fluctuations in the
    signal energy and are unrelated to bird vocalisations.

\end{enumerate}

\subsubsection{Onset/offset detection}

The first phase considers the energy of an input signal. The energy envelope is
the key variable to use in determining the onset and offset of a syllable. The
general idea is that when the energy increases past a threshold, this marks the
start of a syllable candidate and when the energy then falls below the same
threshold, this marks the end of a syllable candidate. To calculate the energy
envelope, the recording is divided into overlapping frames.
Fagerlund~\cite{fagerlund2004automatic} recommends a frame size of 128 samples
and an overlap of 50\%. This corresponds to a frame length of roughly 3ms. Since
distance between syllables can be as short as 20ms, the frame size needs to be
small enough so that there are at least a few frames between candidates to be
able to efficiently discriminate between syllables.

Frames are first windowed using a Hanning window, which minimises the effects of
the discontinuities at the edges of the frames. The energy $E_i$ in the
decibel scale for frame $i$ is calculated as
\begin{equation}
  E_i = 10 \log_{10} \sum_{j=1}^{K} x{(i)}_{j}^2
\end{equation}
where $x{(i)}_j$ is the $j^{\text{th}}$ input signal sample value in the
$i^{\text{th}}$ frame and $K$ is the total number of samples in each frame, in
this case 128. The maximum energy of the entire signal is normalised to 0dB. The
initial noise level estimate is set to be equal to the global minimum value of the
energy envelope and the threshold for the onset and offset detection is set to
half the noise estimate. The flow diagram for onset and offset detection can be
seen in Figure~\ref{fig:syllable_segmentation_flow}. 

\begin{figure}[ht]
  \centering
  \includegraphics[width=\textwidth]{figures/syllable_segmentation_flow.png}
  \caption{Flow chart of the syllable segmentation onset and offset detection
  algorithm. Figure taken
from~\cite{fagerlund2004automatic}.}\label{fig:syllable_segmentation_flow}
\end{figure}

In essence, the algorithm moves through each frame and computes whether the
frame should be assigned to a syllable or not, updating noise and energy
thresholds as it goes. The noise estimate is updated as the average energy of
frames already processed by the algorithm that are not assigned to a syllable.

\subsubsection{Merging syllable candidates}

The algorithm may produce syllable candidates that are very temporally close
together. This may be from the bird producing short bursts of pulses that
constitute the same syllable, or it may be from the border effect. In both cases
the candidates can be considered as belonging to the same syllable and so can be
safely merged together. Candidates that are less than 15ms apart are selected
for merging~\cite{fagerlund2004automatic}. The process of onset/offset detection
and merging can be seen in Figure~\ref{fig:syllable_segmentation_flow}.

\begin{figure}[ht]
  \centering
  \includegraphics[width=\textwidth]{figures/syllable_segmentation_2.png}
  \caption{Figure showing the syllable detection algorithm. Top left shows the
    amplitude varying over time for a sample of Eurasian wren birdsong. Top
    right shows the detected onset (green triangle) and offset (red triangle)
    markers overlaid on the energy envelope in the decibel scale of the sample.
    The dashed magenta line shows the final threshold for syllable detection.
    Bottom left shows the markers after merging has been performed. Bottom right
    shows the detected segmented syllables, labelled from A --- G. Note that the
    next step, removing syllables less than a minimum duration, would remove
    syllable candidate B since it is less than the minimum duration
threshold.}\label{fig:syllable_segmentation}
\end{figure}

One thing to note about the algorithm is that the energy threshold is updated
iteratively as the algorithm moves through the signal. This means that
candidates detected near the start of the sample may not get detected if they
were nearer the end of the signal. This is the case in
Figure~\ref{fig:syllable_segmentation} with syllable candidate B, since its
energy peak is below the final energy threshold, shown in the top right panel of
the figure. This shouldn't pose an issue since candidates less than the minimum
duration threshold are removed, as is the case with candidate B.

Each segmented syllable is then stored as an individual \textit{wav} file in the
same parent directory as the original recording with a filename in the format
\textit{\{XCID\}\_i.wav} where \textit{\{XCID\}} is the XC id of the recording
and \textit{i} is the index of the syllable.

\subsection{Sequence sample generation}\label{ssec:sequence_gen}

Once the syllables have been extracted, they are ready to be combined to form a
sequence. This style of sequence generation will be used for all training in
this work unless otherwise stated.

We first start by grouping syllables by XC id. Then, for each syllable
in each group, the following syllables are appended until a fixed sample length
is reached. If a syllable is added which pushes the sample length over the
limit then the sample is trimmed at the fixed length. If there aren't enough
syllables to reach the fixed length, then the sample is discarded. This process
can be visualised in Figure~\ref{fig:sequence_building}.

\begin{figure}[ht]
  \centering
  \includegraphics[width=\textwidth]{figures/sequence_building_2.png}
  \caption{Figure demonstrating building sequences out of an input signal. The
    top panel shows the input signal with syllables discovered during
    segmentation highlighted in colours. The bottom 3 panels show the first 3
    sequences generated from the input signal. The red dashed line indicates the
    fixed length, in this case $10,000$ samples. Syllable segments that have
    been trimmed as they push the sequence over the fixed length are shown in
    grey. Note that for Sequence 3 its constituent syllables make up exactly
    10,000 samples, and so no trimming was
  necessary.}\label{fig:sequence_building}
\end{figure}

This approach is motivated by Somervuo et al.'s
work~\cite{somervuo2006parametric} in showing that training and classifying
using single syllables returns suboptimal results, whereas using sequences of
syllables gives much improved accuracy. Of course there is a tradeoff here
between training with longer sequences, and hence more information, versus
increased computational demand to perform training.

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.95\textwidth]{figures/syllable_sample_length.png}
  \caption{Histogram showing the frequency of different sample lengths for all
  training syllables. A candidate fixed sequence length is denoted by the red
dashed line.}\label{fig:syllable_sample_lengths}
\end{figure}

From Figure~\ref{fig:syllable_sample_lengths}, we can see most syllable lengths
fall in the region $\left[ 265, 1236 \right]$ (the segmentation algorithm sets a
minimum of 265 samples for candidate syllables). Therefore a sequence of fixed
length about 14000 (denoted by the red dashed line) should contain enough
syllables to capture sufficient information about the vocalisation. The fixed
length, equating to about 300ms duration, also ensures that most sequences
contain at least one full syllable. This approach ensures that the influence of
background noise is kept to a minimum, while still maintaining information
related to the temporal evolution of a particular birdsong.

\section{Extracting features from samples}\label{sec:method:feat_extraction}

Once the sequences have been generated, they are ready to be converted to
features. The following sections describe the feature extraction processes used
in this work.

\subsection{Cepstral coefficients}

The cepstral coefficients on which this thesis focuses are the MFCC and GTCC\@.
Both features are extracted using builtin MATLAB functions \texttt{mfcc} and
\texttt{gtcc} respectively. The functions accept arguments which affect how the
routine processes input signals to calculate the coefficients. To the best of
the author's knowledge, these arguments have not been explored in the field of
birdsong classification problems.

\subsubsection{MFCC}\label{sssec:mfcc}

A potentially overlooked parameter of MFCC extraction with regards to birdsong
classification using MATLAB is the \texttt{BandEdges} argument. This
argument provides the basis of the half-overlapped triangular filters that
\texttt{mfcc} uses. By default, \texttt{BandEdges} is a 42-element vector that
results in a 40-band filter bank constructed using 13 linearly-spaced filters,
with 133.3 Hz separating their centre frequencies, and 27 log-spaced filters,
separated by a factor of 1.0711703 in frequency~\cite{slaney1998auditory}. This
equates to a frequency range that spans approximately 133 Hz to 6864 Hz. This
range is well suited to human speech, of which MFCC is often used as a feature,
but may be less suitable for birdsong which can sometimes sit outside the normal
frequency range of human speech.

To test the \texttt{BandEdges} argument, 7 experiments were devised and
evaluated to see if any improvements to binary bird classification
models utilising MFCC could be made. The experiments are listed in
Table~\ref{table:h1_mfcc_experiments}.

\begin{table}[ht]
\begin{center}
\begin{tabular}{c c c c}
\toprule
Experiment \# & Number of bands & Band type & Approximate frequency range (Hz) \\ [0.5ex]
\midrule
1 & 40 & Mel & 133 --- 6864 \\
2 & 40 & Mel & 50 --- $\text{fs}/2$ \\
3 & 80 & Mel & 50 --- $\text{fs}/2$ \\
4 & 40 & Linear & 50 --- $\text{fs}/2$ \\
5 & 40 & Linear & 133 --- 6864 \\
6 & 40 & A-mel & 133 --- 6864 \\
7 & 40 & Mel & 133 --- 10000 \\
\bottomrule
\end{tabular}
\caption{Description of experiments used for testing H1 with
MFCC.}\label{table:h1_mfcc_experiments}
\end{center}
\end{table}

\subsubsection{Comments on experiment setup}

In the table, `fs' refers to the frequency sampling rate for a given audio file,
usually 44.1 kHz or 48 kHz. $\text{fs}/2$ is the maximum possible value  of the
vector supplied for the \texttt{BandEdges} argument. `A-mel', short for
anti-mel, denotes a set of mel filterbanks that have been inverted, such that
the centre frequencies are closer together at higher frequencies and further
apart at lower frequencies. `Linear' refers to a set of filterbanks that have
been spaced linearly on the frequency scale. This has the effect of transforming
the MFCC coefficients to Linear Frequency Cepstral Coefficients (LFCC). Mel band
edges were generated by first converting a minimum and maximum frequency to the
mel scale using
\begin{equation}\label{eq:mel}
m = 1127 \cdot \ln (1 + \frac{f}{700})
\end{equation}
where $m$ is the corresponding mel frequency for frequency in Hz $f$. A vector of band
edges was created using MATLAB's \texttt{linspace} function with the minimum and
maximum mel-frequency and desired number of bands. The vector was then converted
back to the Hz scale using the inverse of Equation (\ref{eq:mel}) and passed to
the \texttt{BandEdges} argument. Antimel band edges were created by doing the
same process and then flipping the result. Linear band edges were created by
simply applying a \texttt{linspace} function to a minimum and maximum frequency
in Hz. The different band types can be visualised in
Figure~\ref{fig:filterbanks}.

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.7\textwidth]{figures/filterbanks.png}
  \caption{Illustration of the mel-, linear-, and antimel- frequency
  filterbanks. Figure taken from~\cite{lei2009mel}.}\label{fig:filterbanks}
\end{figure}

The experiment design was motivated as follows:

\begin{itemize}

  \item [Exp 1:] These are the defaults provided with the \texttt{mfcc}
    function and acts as a control experiment.

  \item [Exp 2:] A wider frequency range may be able to make use of the
    higher frequency harmonics produced by most birdsong.

  \item [Exp 3:] A wider frequency range means the bands have a larger bandwidth
    and may lose some distinguishing power. Increasing the number of bands may
    help to alleviate this.

  \item [Exp 4:] Since the human auditory system hasn't evolved specifically to be
    able to distinguish birdsong, it's entirely possible that the mel scale
    isn't the optimum scale to use. A linearly spaced set of filterbanks may
    provide a good generalised attempt at distinguishing birdsong.

  \item [Exp 5:] Similar to Experiment 4 but focused on a frequency range that
    more tightly fits around the dominant frequencies produced by bird
    vocalisations.

  \item [Exp 6:] The anti-mel scale has been used with interesting results in
    human speech recognition through telephones~\cite{lei2009mel}. It offers
    more distinguishing power at higher frequencies, and since birdsong is
    typically produced at higher frequencies than human speech, the anti-mel
    scale may be able to better distinguish birdsong.

  \item [Exp 7:] Similar to Experiment 2 but still focused on the main
    frequencies emitted by birds.

\end{itemize}

\subsubsection{GTCC}\label{sssec:gtcc}

Like \texttt{mfcc}, \texttt{gtcc} has a potentially overlooked argument in
\texttt{FrequencyRange}. This two-element row vector specifies the minimum and
maximum values in Hz of the gammatone filter bank used by the \texttt{gtcc}
routine. The default value is \texttt{[50, fs/2]}, which could potentially be
fine-tuned for birdsong related problems since birdsong frequencies do not
typically go below 1 kHz.

To test the \texttt{FrequencyRange} argument, 4 experiments were devised and
evaluated to see if any improvements to binary bird classification
models utilising GTCC could be made. The experiments are listed in
Table~\ref{table:h1_gtcc_experiments}.

\begin{table}[ht]
\begin{center}
\begin{tabular}{c c}
\toprule
Experiment number & Frequency range (Hz) \\ [0.5ex]
\midrule
1 & 50 --- $\text{fs}/2$ \\
2 & 100 --- 7000 \\
3 & 50 --- 10000 \\
4 & 400 --- 6000 \\
\bottomrule
\end{tabular}
\caption{Description of experiments used for testing H1 with
GTCC.}\label{table:h1_gtcc_experiments}
\end{center}
\end{table}

\subsubsection{Comments on experiment setup}

As with the MFCC experiments, `fs' refers to the frequency sampling rate for a
given audio file. $\text{fs}/2$ is the maximum possible value for the
\texttt{FrequencyRange} argument. By default the filter bank is a gammatone
filter bank, which can be visualised in Figure~\ref{fig:gammatone_filterbank}.
Note that it's not possible to change the number of bands used by \texttt{gtcc}.

\begin{figure}[ht]
  \centering
  \includegraphics[width=\textwidth]{figures/gammatone_filterbank.png}
  \caption{Illustration of a typical gammatone
  filterbank.}\label{fig:gammatone_filterbank}
\end{figure}

The experiment design was motivated as follows:

\begin{itemize}

  \item [Exp 1:] These are the defaults provided with the \texttt{gtcc} function
    and acts as a control experiment.

  \item [Exp 2:] This more closely matches the effective frequency range for the
    control experiment for \texttt{mfcc}. This range matches human speech and
    contains most birdsong frequencies.

  \item [Exp 3:] Similar to Experiment 2, but widens the range to account for
    frequencies outside of Experiment 2's range.

  \item [Exp 4:] Similar to Experiment 2, but tightens the range to achieve more
    discriminatory power.

\end{itemize}

\subsection{MRCG}

MATLAB provides no builtin functions for computing a MRCG, or even a
cochleagram, upon which the MRCG is based. To that end, custom code was
developed based on the toolbox developed by Kim and Hahn~\cite{kim2018voice}
which implements MRCG\@. The MRCG routine implemented in~\cite{kim2018voice}
uses a gammatone filter bank with a frequency range of \texttt{[50, 8000]},
which may not be suitable for birdsong problems. The work done in customising
the toolbox for this thesis extends the implementation to work with alternative
frequency ranges for the gammatone filter bank.

It's recommended to include the $\Delta$ and $\Delta\Delta$ values along with
the MRCG feature~\cite{chen2014feature}, so hereafter the term MRCG shall refer
to the base MRCG along with its $\Delta$ and $\Delta\Delta$ values. The same
applies to MFCC and GTCC\@. While the code from~\cite{kim2018voice} includes a
custom implementation of the delta calculation, in this thesis the
\texttt{audioDelta} function was used due to it being a builtin function in
MATLAB and therefore requiring no customisation.

Due to the high dimensionality of the MRCG feature and considering the
computational limitations of the machine upon which the training for this thesis
was performed, the MRCG feature would be best suited to a classifier which is
optimised for working with image data, such as CNNs\@. To that end, it won't
be used with SVMs in this work.

\section{Training models}\label{sec:training}

To create the training MATLAB cell array, sequences for each class are
generated and stored in a cell array. A maximum limit of 40 sequences per
individual is imposed to ensure enough variation while keeping training times to
a minimum. The number of training sequences used for both classes can be seen in
Table~\ref{table:training_samples}.

\begin{table}[ht]
\begin{center}
\begin{tabular}{c c c}
\toprule
Class & Individuals & Sequences \\ [0.5ex]
\midrule
Common blackbird (\textit{TURMER}) & 28 & 1107 \\
Common nightingale (\textit{LUSMEG}) & 28 & 1200 \\
\bottomrule
\end{tabular}
\caption{Number of training sequences for used for both
classes.}\label{table:training_samples}
\end{center}
\end{table}

The training cell is formed by vertically concatenating the two cell arrays
containing the training sequences to form a $2307 \times 1$ cell array. The
training labels are contained in a $2307 \times 1$ categorical array with a
\textit{TURMER} category labelling a blackbird sequence and \textit{LUSMEG}
labelling a nightingale sequence.

All training is performed on an Apple MacBook Pro (late 2013) running OSX
11.5.2.

\subsection{SVMs}

All SVM models are implemented using MATLAB's \texttt{fitcsvm}
function provided in the Statistics and Machine Learning Toolbox.

SVMs require each training sample to be a row vector. To convert the training
cell array to the required matrix format, a function can be applied to each cell
using the \texttt{cellfun} function to extract the desired feature and flatten
the output to a row vector. A simple routine can then be applied to convert the
cell array to a matrix of numerical data, the required format for
\texttt{fitcsvm}. The training labels cell array mentioned in the previous
section requires no transformation.

\subsubsection{Linear}

Linear SVMs are trained by supplying a \texttt{KernelFunction} argument set to
\texttt{linear} to the \texttt{fitcsvm} function. Linear SVMs are reasonably
simple to fit as they have fewer hyperparameters than SVMs utilising other kernel
functions. The key hyperparameter to tune here is \texttt{BoxConstraint}, which
controls the trade-off between maximising the margin between classes and
minimise the classification error. This corresponds to the constant $C$ in
Equation (\ref{eq:svm_nonlinear_mapping_slack}). The \texttt{Standardize}
parameter controls whether data is standardized before fitting, and can also
affect the performance of the model. In this work, KFold cross-validation (KF)
was selected to optimise the parameters, with $K=7$ folds. This is motivated by
other research taking the same approach with promising
results~\cite{ramashini2022robust}. MATLAB provides builtin ways to optimise
hyperparameters, which can be achieved by passing a \texttt{cvpartition} to the
\texttt{HyperparameterOptimizationOptions} argument and supplying an
\texttt{OptimizeHyperparameters} argument set to the name of the desired
hyperparameters to tune to \texttt{fitcsvm}, in this case a cell array of
\texttt{BoxConstraint} and \texttt{Standardize}.

Figure~\ref{fig:linear_optim} shows where the optimum values of
\texttt{BoxConstraint} and \texttt{Standardize} lie for a linear SVM\@. As can
be seen, the \texttt{Standardize} parameter makes a slight difference on model
performance, whereas \texttt{BoxConstraint} makes a significant difference, with
lower values preferred. A lower value means that the model has greater
flexibility in misclassifying training samples for the benefit of better
generalisation.

\begin{figure}[ht]
  \centering
  \includegraphics[width=\textwidth]{figures/linear_optim.png}
  \caption{Plot showing optimum values of the \texttt{BoxConstraint} and
  \texttt{Standardize} parameters when fitting a linear SVM.\ The optimum values
are located where the objective function is at a
minimum.}\label{fig:linear_optim}
\end{figure}

\subsubsection{RBF}

RBF SVMs are able to fit more complex decision boundaries by projecting data
into a higher dimensional space, which is controlled by the hyperparameter
\texttt{KernelScale}. RBF SVMs are also sensitive to the \texttt{BoxConstraint}
parameter. 7-fold cross validation was performed using the same routine as the
linear SVM but this time by supplying an \texttt{OptimizeHyperparameters}
argument set to \texttt{auto} to \texttt{fitcsvm}. A value of \texttt{auto}
means that both \texttt{KernelScale} and \texttt{BoxConstraint} are optimised
during training.

Figure~\ref{fig:rbf_optim} shows where the optimum values of
\texttt{BoxConstraint} and \texttt{KernelScale} lie for a RBF SVM\@. Both
parameters are shown to have an impact on the model performance, with high
values of \texttt{BoxConstraint} and mid-range values of \texttt{KernelScale}
preferred. The \texttt{KernelScale} controls the width of the kernel, with a
lower value resulting in a narrow kernel which can be interpreted as a more
`wiggly' decision boundary. Higher values of \texttt{KernelScale} result in
a smoother decision boundary. This corresponds exactly to the bias-variance
tradeoff, whereby training fit and generalisation directly compete with each
other.

\begin{figure}[ht]
  \centering
  \includegraphics[width=\textwidth]{figures/rbf_optim.png}
  \caption{Plot showing optimum values of the \texttt{BoxConstraint} and
  \texttt{KernelScale} parameters when fitting a RBF SVM.\ The optimum values
are located where the objective function is at a
minimum.}\label{fig:rbf_optim}
\end{figure}

\subsection{Neural networks}

All NN models are implemented using tools provided in MATLAB's Deep Learning
Toolbox. NN models are trained by first generating an array of sequential layers
that describe the network using builtin layer structures such as
\texttt{reulLayer} (which denotes a layer of ReLU activations), and
\texttt{convolutional2dLayer} (which denotes a 2D convolutional layer). The
layers are then passed to the \texttt{trainNetwork} function along with a set of
options that describe hyperparameters such as the mini-batch size and the
optimisation algorithm used, as well as the training input and corresponding
labels.

The training input for NN models depends on the type of NN\@. Some can work
directly on cell arrays whereas others require training data in numerical matrix
format. For all NN training runs, 5\% of the training set was set aside for
validation. If the validation accuracy was seen to plateau during training then
this was considered grounds for early stopping of training.

\subsubsection{CNN}\label{sssec:method:cnn}

Unlike SVMs, which expect a training sample to be a row vector, CNNs can work
directly on image data e.g.\ a training sample in matrix format of dimensions
$w \times h \times c$ where $w$ is the width, $h$ is the height, and $c$ is the
number of channels, 1 for greyscale images and 3 for colour (RGB) images. The
features used in this work therefore require no transformation to work with CNN
since e.g.\ the MFCC representation of a signal can be interpreted as a
greyscale image of dimensions $p \times n \times 1$ where $p$ is the number of
frames and $n$ is the number of coefficients (including $\Delta$ and
$\Delta\Delta$). For the fixed sequence length described in
Section~\ref{sec:segmentation}, this equates to an input of dimensions $28
\times 39 \times 1$ for the MFCC representation.

The key hyperparameters regarding NNs relate to model architecture, such as
number of hidden layers and activation function. CNNs have further
hyperparameters to tune, such as kernel size and stride, padding type, and
pooling size. Optimising hyperparameters for CNNs can take a
considerable amount of time and computational resources, so we look to the
literature to best inform our model architecture. The following architectures are
shown to have promising results and/or are related to the problem at hand. They
provide the motivation for the CNN architectures tested in this work.

\begin{enumerate}

  \item Ruff et al.~\cite{ruff2020automated} proposed a  CNN, for their birdsong
    classification research with 7 classes. Their model contained
    6 layers, of which 4 were convolutional. They used the rectified linear unit
    (ReLU) as an activation function. To avoid overfitting, they used dropout
    layers~\cite{srivastava2014dropout} with $p=0.2$ for
    convolutional layers and $p=0.5$ for the fully connected layers.

  \item Kahl et al.~\cite{kahl2017large} proposed a CNN for their work focused
    on a very large number of classes (1500). Although the difference in numbers
    of classes between their work and this work is significant, the concepts
    should apply to binary classification problems too. In their work they
    tested 3 models with varying numbers of hidden layers and kernel sizes.
    Their model number 3, comprising 8 layers of which 5 were convolutional,
    was shown to have superior results for smaller numbers of classes, indeed
    they mention that shallow models perform better on small class selections.
    All of their proposed models use batch normalisation~\cite{ioffe2015batch},
    exponential linear units (ELU)~\cite{clevert2015fast} as the activation
    function, and He-initialization~\cite{he2015delving} for their weighted
    layers. Dropout layers with $p=0.5$ were added to each weighted layer before
    the final dense layer. An initial learning rate of 0.1 is set, which
    decreases linearly after each epoch to a final value of 0.0001.

  % \item Berger et al.~\cite{berger2018bird} also proposed a simple CNN for their
  %   work in bird detection. Although bird detection and binary bird
  %   classification have their differences, they are both binary classification
  %   problems and so a successful architecture for bird detection may be
  %   effective for binary bird classification too. Their architecture consists of
  %   a 7 layer network, of which 4 are convolutional. They use LeakyReLu as
  %   an activation function for all layers

\end{enumerate}

To test against a baseline, a highly simplistic CNN architecture will be
implemented and tested against these more complex designs. The proposed CNN
architectures are listed in Table~\ref{table:cnn_architectures}.

\begin{table}[hp]
\begin{center}
\begin{tabular}{l l l}
\toprule
Model 0 & Model 1 (Ruff~\cite{ruff2020automated}) & Model 2 (Kahl~\cite{kahl2017large}) \\[0.5ex]
2 weighted layers & 6 weighted layers & 8 weighted layers \\[0.5ex]
ReLU activation & ReLU activation & ELU activation \\[0.5ex]
Glorot initialization & Glorot initialization & He initialization \\[0.5ex]
32.5k parameters & 3.3m parameters & 23.6m parameters \\[0.5ex]
% $\sim$3s per epoch & $\sim$60s per epoch & $\sim$165s per epoch \\[0.5ex]
\midrule
\textbf{Conv1, $20 \times 5 \times 5$} &
\textbf{Conv1, $32 \times 3 \times 3$} &
\textbf{Conv1, $32 \times 7 \times 7$, Stride 2} \\
& MaxPooling, Size 2 & MaxPooling, Size 2 \\
& Dropout, $p=0.2$ & \\[1ex]
\textbf{DenseLayer, 2 Units} &
\textbf{Conv2, $32 \times 3 \times 3$} &
\textbf{Conv2, $128 \times 5 \times 5$, Stride 1} \\
Softmax Output & MaxPooling, Size 2 & MaxPooling, Size 2 \\
& Dropout, $p=0.2$ & \\[1ex]
& \textbf{Conv3, $64 \times 3 \times 3$} &
\textbf{Conv3, $256 \times 3 \times 3$, Stride 1} \\
& MaxPooling, Size 2 & MaxPooling, Size 2 \\
& Dropout, $p=0.2$ & \\[1ex]
& \textbf{Conv4, $64 \times 3 \times 3$} &
\textbf{Conv4, $512 \times 3 \times 3$, Stride 1} \\
& MaxPooling, Size 2 & MaxPooling, Size 2 \\
& Dropout, $p=0.2$ & \\[1ex]
& \textbf{DenseLayer, 64 Units} &
\textbf{Conv4, $512 \times 3 \times 3$, Stride 1} \\
& Dropout, $p=0.5$ & MaxPooling, Size 2 \\[1ex]
& \textbf{DenseLayer, 2 Units} &
\textbf{DenseLayer, 512 Units} \\
& Softmax output & Dropout, $p=0.5$ \\[1ex]
& & \textbf{DenseLayer, 512 Units} \\
& & Dropout, $p=0.5$ \\[1ex]
& & \textbf{DenseLayer, 2 Units} \\
& & Softmax output \\[1ex]
\bottomrule
\end{tabular}
\caption{Candidate CNN architectures. All models use batch normalisation after
each weighted layer and all convolutional layers use shape preserving
padding. All models use the `Adam' optimiser with an $l_2$ regularisation of
$0.0001$.}\label{table:cnn_architectures}
\end{center}
\end{table}

\subsubsection{RNN}\label{sssec:method:rnn}

Unlike CNNs, RNNs can work with variable sized inputs. This makes them ideal for
working with sequential data such as videos (consisting of sequential frames) and
potentially birdsong (consisting of sequential syllables). Since we don't need
fixed length inputs as training samples, we can utilise a simplified form of the
sequence generation method described in Section~\ref{ssec:sequence_gen} for
fixed length sequences (FLS) by simply concatenating $n$ syllables together to
form a variable length sequence (VLS). We select $n$ by choosing a value such
that a VLS has a 50\% chance of being shorter than the FLS\@.
Figure~\ref{fig:vls_lengths} shows a histogram of the sample lengths for the VLS
for $n=5$. The proportion of VLS with a length less than 14,000 is 0.5044, i.e.\
the probability that a VLS is shorter than the FLS is 50\%, as desired.

\begin{figure}[ht]
  \centering
  \includegraphics[width=\textwidth]{figures/vls_lengths.png}
  \caption{Histogram showing the frequency of different sample lengths for VLS
  with $n=5$. The red dashed line shows the length of the
FLS.}\label{fig:vls_lengths}
\end{figure}

With this form of sequence generation, the number of training samples for
each class is shown in Table~\ref{table:vls_training_samples}. Note that these
samples are only used for RNN\@.

\begin{table}[ht]
\begin{center}
\begin{tabular}{c c c}
\toprule
Class & Individuals & Sequences \\ [0.5ex]
\midrule
Common blackbird (\textit{TURMER}) & 28 & 1062 \\
Common nightingale (\textit{LUSMEG}) & 28 & 1200 \\
\bottomrule
\end{tabular}
\caption{Number of training sequences used for both
classes for VLS used for RNN.}\label{table:vls_training_samples}
\end{center}
\end{table}

MATLAB can receive sequential data in different input formats, such as vector
sequences and 2D image sequences. To trigger a sequence input, the first layer
of the network must be a \texttt{sequenceInputLayer}. The layer accepts an
\texttt{inputSize} argument, which determines the sequence format. For a vector
sequence, \texttt{inputSize} is a scalar corresponding to the number of
features. For 2D image sequences, \texttt{inputSize} is a vector of three
elements $\begin{bmatrix} h & w & c \end{bmatrix} $, where $h$ is the image
height, $w$ is the image width, and $c$ is the number of channels of the image.
The formulation of the VLS lends itself well to the vector sequence input
format. To illustrate this, we will concern ourselves only with MFCC, but the
concepts can easily be applied to other representations in the frequency domain
where each element can be thought of as a T-F unit, such as GTCC and MRCG\@.

The MFCC for an entire sequence is first extracted. Supposing we extract 13 MFCC
coefficients (ignoring $\Delta$ and $\Delta\Delta$ values for now), then the
entire sequence can then be thought of as a sequence of 13 dimensional vectors,
evolving through time. This can be visualised in
Figure~\ref{fig:mfcc_vector_sequence}. An alternative way to visualise the MFCC
as sequential vectors is presented in Figure~\ref{fig:mfcc_vector_sequence_2}.

\begin{figure}[ht]
  \centering
  \includegraphics[width=\textwidth]{figures/mfcc_vector_sequence_labelled_2.png}
  \caption{MFCC representation of a VLS showing sequential vectors. The $13
  \times 12$ matrix can be thought of as a sequence of length 12 of 13
dimensional vectors. The first two vectors are shown in the figure using
dashed lines.}\label{fig:mfcc_vector_sequence}
\end{figure}

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.8\textwidth]{figures/mfcc_vector_sequence_2.png}
  \caption{Plot of the last 6 MFCC coefficients of a VLS changing with each
  timestep.}\label{fig:mfcc_vector_sequence_2}
\end{figure}

This approach has the benefit that the training cell needs little
manipulation to be compatible with MATLAB's \texttt{trainNetwork}
function used to train a NN\@. The function can accept a training cell array
where each cell is a $p \times s$ numerical matrix, where $s$ is the number
of timesteps in the sequence, and $p$ is the number of features. In the
example shown in Figure~\ref{fig:mfcc_vector_sequence}, $s=12$ and $p=13$.
However, this method will treat the dimensions as independent of each other
which will mean that the network isn't able to learn cross dimensional
features. In other words, only 1 dimensional convolutions may be applied.
This may mean that the network misses out on learning from potentially
useful patterns in the signal.

An alternative formulation would be to consider sequences of 2D images. Here,
the MFCC representation is extracted for each of $n$ syllables in a sequence
separately. They are then stacked together to form a sequence of $n \times$ 2D
`images' (MFCC representations). This sequence can then be fed as input to the
network. For this approach, the network expects a training input as a $h \times
w \times c \times s$ numerical array where $h$, $w$, and $c$ correspond to the
height, width, and number of channels of the images, respectively, and $s$ is
the sequence length. Here, $s=n$, $c=1$ since the images can be thought of as
greyscale and $h$ will be the number of MFCC coefficients. The big drawback of
this method is that $w$ will need to be consistent across all images in the
training input. In other words, all syllables will need to be the same length.
This can be achieved with rudimentary processes like padding or looping a
syllable until it is of a desired length, but to achieve less noisy results it's
likely that this would require more advanced techniques like dynamic time
warping. The large scope of this approach means that it will be left to be
tackled in future work. A big benefit of this approach is that the network will
be able to learn cross dimensional features by applying 2D convolutions. This
would complement the temporal features learned by recurrent layers of the
network well.

\subsubsection{Bidirectionality}

If the full sequences are known at prediction time, as is the case in this work,
then the network can benefit from employing bidirectional RNN layers. This means
that information can flow backward and forward through the network, meaning that
predictions at a certain timestep can be influenced by future predictions. At
the time of writing this work, bidirectional LSTM layers exist in MATLAB
(\texttt{bilstmLayer}), but bidirectional GRU layers do not.

To test RNN architectures for sequential vectors, we propose 3 models.
Given that the literature is reasonably thin in terms of implementing RNN for
birdsong classification problems, we propose 3 simple models to establish a
baseline. The models can be seen in Table~\ref{table:rnn_seq_vector}.

\begin{table}[ht]
\begin{center}
\begin{tabular}{l l l}
\toprule
Model 1 & Model 2 & Model 3 \\ [0.5ex]
2 weighted layers & 4 weighted layers & 4 weighted layers \\[0.5ex]
112.4k parameters & 142.5k parameters & 59.8k parameters \\[0.5ex]
% $\sim$16s per epoch & $\sim$3s per epoch & $\sim$3s per epoch \\[0.5ex]
\midrule
\textbf{BiLSTM, 100 Units} & \textbf{Conv1}, $32 \times 3$ & \textbf{Conv1}, $32 \times 3$ \\
Dropout, $p=0.2$ & MaxPooling, Size 2 & MaxPooling, Size 2 \\
& Dropout, $p=0.2$ & Dropout, $p=0.2$ \\[1ex]
\textbf{DenseLayer, 2 Units} & \textbf{Conv2}, $64 \times 3$ & \textbf{Conv2}, $64 \times 3$ \\
Softmax Output & Global Average Pooling & Global Average Pooling \\
& Dropout, $p=0.2$ & Dropout, $p=0.2$ \\[1ex]
& \textbf{BiLSTM, 100 Units} & \textbf{GRU, 100 Units} \\
& Dropout, $p=0.5$  & Dropout, $p=0.5$  \\[1ex]
& \textbf{DenseLayer, 2 Units} & \textbf{DenseLayer, 2 Units} \\
& Softmax Output & Softmax Output \\
\bottomrule
\end{tabular}
\caption{Candidate RNN and CRNN architectures for sequential vector sequences.
  All models use batch normalisation after each weighted layer and all
  convolutional layers use shape preserving padding. All models use the `Adam'
  optimiser with an $l_2$ regularisation of $0.0001$, with Glorot initialization
and ReLU activation.}\label{table:rnn_seq_vector}
\end{center}
\end{table}

One thing to note with RNN in MATLAB is that although the sequences can
be of varying length, the lengths of each sequence in each mini-batch provided
to the training routine need to be the same length. Padding is automatically
added by the software to each sequence in a mini-batch so that they are all of
the same length, usually by padding sequences so that they are the same length
as the longest sequence in the mini-batch. To that end, it's recommended to sort
the sequences by length before passing them to the training routine and ensuring
that the software doesn't shuffle the samples. This ensures that the amount of
padding is kept to a minimum which is preferable since padding introduces noise
to the network. A mini-batch size of 100 was used for all RNN models.

\section{Evaluating models}

After the recordings from each class reserved for testing have undergone
pre-processing and segmentation, the segmented syllables undergo the same
routine described in Section~\ref{ssec:sequence_gen} to generate the testing
sequences. The number of testing sequences used for both classes can be seen in
Table~\ref{table:testing_samples}.

\begin{table}[ht]
\begin{center}
\begin{tabular}{c c c}
\toprule
Class & Individuals & Sequences \\ [0.5ex]
\midrule
Common blackbird (\textit{TURMER}) & 9 & 360 \\
Common nightingale (\textit{LUSMEG}) & 10 & 400 \\
\bottomrule
\end{tabular}
\caption{Number of testing sequences used for both
classes.}\label{table:testing_samples}
\end{center}
\end{table}

A testing cell is created in the same way as the training cell and is used for
evaluating all models in this thesis.

\subsection{AUC}

As mentioned in Section~\ref{sec:eval_metrics}, the AUC metric is commonly used
for binary classification problems and has been frequently used as a metric in
birdsong classification problems. Further to the advantages listed previously,
the AUC benefits from being unaffected from class imbalances. Although the
positive and negative classes are only slightly imbalanced in this work (see
Table~\ref{table:training_samples}), it makes sense to use a metric that is
unaffected by class imbalance all the same.

The AUC is calculated using the \texttt{perfcurve} function which checks the
class prediction scores returned by the model against the true labels.

\subsection{Classification accuracy}

Since the classes are only slightly imbalanced, the classification accuracy
provides a reasonably unbiased, quick, and easily interpretable snapshot as to
how well a model has performed. It is calculated as
\begin{equation}
A = \frac{1}{N}\sum_{i=1}^{N} \mathbb{I}[\bar{y}_i = y_i]
\end{equation}
where $N$ is the number of test samples, $\bar{y}$ is a predicted label, $y$
is the groundtruth label, and $\mathbb{I}$ is the indicator function.

\section{Chapter summary}

In this chapter we started by describing ways in which audio recordings to be
used for experiments in this thesis were collected and pre-processed. We
described in detail the steps involved in segmenting the processed audio to be
used as training inputs. The following section lists the experiments that are
performed in this work related to feature extraction. Lastly, we explore the
different models used herein and discuss how the models will be evaluated.
